{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data. #\n",
    "* First convert the xlsb files to csv format.\n",
    "* Drop samples without a TARGET value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('development_sample.csv', header=0, index_col=None,low_memory=False)\n",
    "dftest = pd.read_csv('assessment_sample.csv', header=0, index_col=None,low_memory=False)\n",
    "#catColumns = df.select_dtypes(include=['object']).columns\n",
    "#df.loc[:,catColumns]= df[catColumns].astype('object')\n",
    "df.dropna(subset=['TARGET'],inplace=True)\n",
    "dfeature = df.drop( ['TARGET','ID2'],axis=1).copy()\n",
    "dfeature_test = dftest.drop( ['ID2'],axis=1).copy()\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hack the data types and unique values\n",
    "pd.set_option('display.max_rows', 500)\n",
    "desc = pd.DataFrame()\n",
    "desc['dtypes'] = dfeature.dtypes\n",
    "desc['nuniques'] = dfeature.nunique()\n",
    "desc['count'] = dfeature.count()\n",
    "desc['value set'] = dfeature.apply(lambda x:set(x), axis=0)\n",
    "desc.to_csv('develop_desc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hack the test data set\n",
    "desc = pd.DataFrame()\n",
    "desc['dtypes'] = dfeature_test.dtypes\n",
    "desc['nuniques'] = dfeature_test.nunique()\n",
    "desc['count'] = dfeature_test.count()\n",
    "desc['value set'] = dfeature_test.apply(lambda x:set(x), axis=0)\n",
    "desc.to_csv('assessment_desc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# explore the data with hist and pdf plot\n",
    "import matplotlib.pyplot as plt\n",
    "for col in dfeature.columns[0:]:\n",
    "    try:\n",
    "        if dfeature.dtypes[col] != dfeature.dtypes['HOUR']:\n",
    "            plt.figure()\n",
    "            plt.hist(dfeature[col])\n",
    "            plt.title(col)\n",
    "        else:\n",
    "            ser = dfeature[col]\n",
    "            sergp = ser.groupby(ser.values).size().sort_values(ascending=False)\n",
    "            plt.figure()\n",
    "            plt.plot(sergp.values)\n",    
    "            plt.title(col)\n",
    "            plt.xticks(range(len(sergp)), sergp.index, rotation='vertical')\n",
    "    except ValueError:\n",
    "            print(col)\n",
    "plt.hist(dfeature.COMPFIELD.dropna())\n",
    "plt.title('COMPFIELD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the initial raw data sets\n",
    "* Ordinal categorical variable should be converted to numerical feature (maybe useful)\n",
    "* Fill nan values\n",
    "    * median for float, \n",
    "    * mode for integer,\n",
    "    * 'Missing' for categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# try to convert some ordinal catetorical variables to numerical\n",
    "def str2float(x):\n",
    "    #try:\n",
    "#       try:\n",
    "#           if np.isnan(x):\n",
    "#               return x\n",
    "#       except TypeError:\n",
    "#               pass\n",
    "        if isinstance(x,float) or isinstance(x,int):\n",
    "            return x\n",
    "        if 'low-' in x:\n",
    "            return (float(x.split('low-')[1]) - 1) * 0.5\n",
    "        if '>=' in x:\n",
    "            return (float(x.split('>=')[1]) + 1) * 1.5\n",
    "        if '<=' in x:\n",
    "            return (float(x.split('<=')[1]) - 1) * 0.5\n",
    "        if '>' in x:\n",
    "            return (float(x.split('>')[1]) + 1) * 1.5\n",
    "        if '<' in x:\n",
    "            return (float(x.split('<')[1]) - 1) * 0.5\n",
    "        if '-' in x:\n",
    "            if x.split('-')[1] == '':\n",
    "                return np.nan\n",
    "            else:\n",
    "                return ( float(x.split('-')[0]) + float(x.split('-')[1]) )* 0.5\n",
    "        if 'N\\\\A' in x:\n",
    "                return np.nan\n",
    "        return float(x)\n",
    "    #except ValueError:\n",
    "    #    print(x)\n",
    "dfnew = pd.DataFrame()\n",
    "for col in dfeature.select_dtypes(include=['object']).columns:\n",
    "    #print(col)\n",
    "    try:\n",
    "        dfeature.loc[:, col+'numerical'] = dfeature[col].apply(str2float)\n",
    "        dfeature_test.loc[:, col+'numerical'] = dfeature_test[col].apply(str2float)\n",
    "        #dfeature.loc[:, col] = dfeature[col].apply(str2float)\n",
    "        #dfeature_test.loc[:, col] = dfeature_test[col].apply(str2float)\n",
    "    except ValueError:\n",
    "        print('Error',col)\n",
    "dfeature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#fill nan values, different strategies for different data types: median for float, mode for integer, 'Missing' for categorical\n",
    "def fill_nan(df):\n",
    "    fillFloatColumns = df.select_dtypes(include=['float']).median()\n",
    "    fillIntColumns = df.select_dtypes(include=['int']).mode().iloc[0]\n",
    "    fillCatColumns = pd.Series( 'Missing', index=df.select_dtypes(include=['object']).columns)\n",
    "    fillValue = fillFloatColumns.append(fillIntColumns).append(fillCatColumns)\n",
    "    #fillValue\n",
    "    df = df.fillna(value=fillValue)\n",
    "    return df\n",
    "\n",
    "dfeature      = fill_nan(dfeature)\n",
    "dfeature_test = fill_nan(dfeature_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcorr = dfeature.corr()\n",
    "colinearList = []\n",
    "for idx in dfcorr.index:\n",
    "    for col in dfcorr.columns:\n",
    "        if dfcorr.loc[idx,col] > 0.9 and idx!=col:\n",
    "            colinearList.append( [set([idx,col]) , dfcorr.loc[idx,col]] )\n",
    "(colinearList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model construction\n",
    "* the main model is xgboost(GBDT)\n",
    "* try to use the leaf id of xgboost/randomoforest as the input of logisticRegression/SVM (need more parameter tuning)\n",
    "* try ensemble some classic models such as MLP,SVM,NaiveBayesian and KNN (need more parameter tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     22,
     25,
     41,
     51,
     83
    ]
   },
   "outputs": [],
   "source": [
    "# feature engineering as a transformer to avoid touching the TARGET information while calculate woe\n",
    "from sklearn.base import BaseEstimator\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder,MinMaxScaler,StandardScaler,QuantileTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif,SelectFromModel\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "import category_encoders as ce\n",
    "from tempfile import mkdtemp\n",
    "\n",
    "\n",
    "def lap_div(x,y):\n",
    "    #laplace division in case 0/0\n",
    "    return (x+1)/(y+1)\n",
    "class derivesFeatureGenerator(BaseEstimator):\n",
    "    #calculate woe\n",
    "    def __init__(self):\n",
    "        self.woeDict = {}\n",
    "    def fit(self, X, y):\n",
    "        for col in X.select_dtypes(include=['object']).columns:\n",
    "            xtab = pd.crosstab(X[col],y)\n",
    "            xtab['woe'] = np.log(  lap_div( xtab[1],xtab[1].sum()) /  lap_div(xtab[0],xtab[0].sum()) )\n",
    "            self.woeDict[col] = xtab['woe'].to_dict()\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        for col in X.select_dtypes(include=['object']).columns:\n",
    "            mapper = self.woeDict[col]\n",
    "            nafill = np.median(list(mapper.values()))\n",
    "            X[col+'_woe'] = X[col].map( lambda x: mapper.setdefault(x, nafill) )\n",
    "        return X\n",
    "class clusterTransformer(BaseEstimator):\n",
    "    #try to use the leaf id from xgboost or randomForest as  inputs of final estimator, make a chain of xgboost/RF + LR/SVM\n",
    "    def __init__(self,estimator):\n",
    "        self.clf = estimator\n",
    "    def fit(self, X, y):\n",
    "        self.clf.fit(X,y)\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        Xleaf = self.clf.apply(X) \n",
    "        return Xleaf\n",
    "class ensembleModel(BaseEstimator):\n",
    "    # use a emsemble Model as the final estimator\n",
    "    def __init__(self,estimators=[]):\n",
    "        self.clfs = estimators\n",
    "    def fit(self, X, y):\n",
    "        for clf in self.clfs:\n",
    "            print('training', type(clf),datetime.now().strftime('%H:%M:%S'))\n",
    "            clf.fit(X,y)\n",
    "        return self\n",
    "    def predict(self, X, y=None):\n",
    "        ypre = pd.DataFrame()\n",
    "        mid = 0\n",
    "        for clf in self.clfs:\n",
    "            print('predicting', type(clf),datetime.now().strftime('%H:%M:%S'))\n",
    "            ypre[mid] = clf.predict(X)\n",
    "            mid += 1\n",
    "        return ypre\n",
    "    def predict_proba(self, X, y=None):\n",
    "        yproba = pd.DataFrame()\n",
    "        mid = 0\n",
    "        for clf in self.clfs:\n",
    "            print('predicting_proba', type(clf),datetime.now().strftime('%H:%M:%S'))\n",
    "            yproba[mid] = clf.predict_proba(X)[:,1]\n",
    "            mid += 1\n",
    "        return yproba\n",
    "    def transform(self, X, y=None):\n",
    "        yproba = pd.DataFrame()\n",
    "        mid = 0\n",
    "        for clf in self.clfs:\n",
    "            yproba[mid] = clf.predict_proba(X)[:,1]\n",
    "            mid += 1\n",
    "        return yproba\n",
    "class ensembleWithStrategy(BaseEstimator):\n",
    "    # use max or min to vote\n",
    "    def __init__(self,estimators=[],operator=None):\n",
    "        self.clfs = estimators\n",
