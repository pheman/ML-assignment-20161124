{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data. #\n",
    "* First convert the xlsb files to csv format.\n",
    "* Drop samples without a TARGET value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('development_sample.csv', header=0, index_col=None,low_memory=False)\n",
    "dftest = pd.read_csv('assessment_sample.csv', header=0, index_col=None,low_memory=False)\n",
    "#catColumns = df.select_dtypes(include=['object']).columns\n",
    "#df.loc[:,catColumns]= df[catColumns].astype('object')\n",
    "df.dropna(subset=['TARGET'],inplace=True)\n",
    "dfeature = df.drop( ['TARGET','ID2'],axis=1).copy()\n",
    "dfeature_test = dftest.drop( ['ID2'],axis=1).copy()\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hack the data types and unique values\n",
    "pd.set_option('display.max_rows', 500)\n",
    "desc = pd.DataFrame()\n",
    "desc['dtypes'] = dfeature.dtypes\n",
    "desc['nuniques'] = dfeature.nunique()\n",
    "desc['count'] = dfeature.count()\n",
    "desc['value set'] = dfeature.apply(lambda x:set(x), axis=0)\n",
    "desc.to_csv('develop_desc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hack the test data set\n",
    "desc = pd.DataFrame()\n",
    "desc['dtypes'] = dfeature_test.dtypes\n",
    "desc['nuniques'] = dfeature_test.nunique()\n",
    "desc['count'] = dfeature_test.count()\n",
    "desc['value set'] = dfeature_test.apply(lambda x:set(x), axis=0)\n",
    "desc.to_csv('assessment_desc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# explore the data with hist and pdf plot\n",
    "import matplotlib.pyplot as plt\n",
    "for col in dfeature.columns[0:]:\n",
    "    try:\n",
    "        if dfeature.dtypes[col] != dfeature.dtypes['HOUR']:\n",
    "            plt.figure()\n",
    "            plt.hist(dfeature[col])\n",
    "            plt.title(col)\n",
    "        else:\n",
    "            ser = dfeature[col]\n",
    "            sergp = ser.groupby(ser.values).size().sort_values(ascending=False)\n",
    "            plt.figure()\n",
    "            plt.plot(sergp.values)\n",    
    "            plt.title(col)\n",
    "            plt.xticks(range(len(sergp)), sergp.index, rotation='vertical')\n",
    "    except ValueError:\n",
    "            print(col)\n",
    "plt.hist(dfeature.COMPFIELD.dropna())\n",
    "plt.title('COMPFIELD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the initial raw data sets\n",
    "* Ordinal categorical variable should be converted to numerical feature (maybe useful)\n",
    "* Fill nan values\n",
    "    * median for float, \n",
    "    * mode for integer,\n",
    "    * 'Missing' for categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# try to convert some ordinal catetorical variables to numerical\n",
    "def str2float(x):\n",
    "    #try:\n",
    "#       try:\n",
    "#           if np.isnan(x):\n",
    "#               return x\n",
    "#       except TypeError:\n",
    "#               pass\n",
    "        if isinstance(x,float) or isinstance(x,int):\n",
    "            return x\n",
    "        if 'low-' in x:\n",
    "            return (float(x.split('low-')[1]) - 1) * 0.5\n",
    "        if '>=' in x:\n",
    "            return (float(x.split('>=')[1]) + 1) * 1.5\n",
    "        if '<=' in x:\n",
    "            return (float(x.split('<=')[1]) - 1) * 0.5\n",
    "        if '>' in x:\n",
    "            return (float(x.split('>')[1]) + 1) * 1.5\n",
    "        if '<' in x:\n",
    "            return (float(x.split('<')[1]) - 1) * 0.5\n",
    "        if '-' in x:\n",
    "            if x.split('-')[1] == '':\n",
    "                return np.nan\n",
    "            else:\n",
    "                return ( float(x.split('-')[0]) + float(x.split('-')[1]) )* 0.5\n",
    "        if 'N\\\\A' in x:\n",
    "                return np.nan\n",
    "        return float(x)\n",
    "    #except ValueError:\n",
    "    #    print(x)\n",
    "dfnew = pd.DataFrame()\n",
    "for col in dfeature.select_dtypes(include=['object']).columns:\n",
    "    #print(col)\n",
    "    try:\n",
    "        dfeature.loc[:, col+'numerical'] = dfeature[col].apply(str2float)\n",
    "        dfeature_test.loc[:, col+'numerical'] = dfeature_test[col].apply(str2float)\n",
    "        #dfeature.loc[:, col] = dfeature[col].apply(str2float)\n",
    "        #dfeature_test.loc[:, col] = dfeature_test[col].apply(str2float)\n",
    "    except ValueError:\n",
    "        print('Error',col)\n",
    "dfeature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#fill nan values, different strategies for different data types: median for float, mode for integer, 'Missing' for categorical\n",
    "def fill_nan(df):\n",
    "    fillFloatColumns = df.select_dtypes(include=['float']).median()\n",
    "    fillIntColumns = df.select_dtypes(include=['int']).mode().iloc[0]\n",
    "    fillCatColumns = pd.Series( 'Missing', index=df.select_dtypes(include=['object']).columns)\n",
    "    fillValue = fillFloatColumns.append(fillIntColumns).append(fillCatColumns)\n",
    "    #fillValue\n",
    "    df = df.fillna(value=fillValue)\n",
    "    return df\n",
    "\n",
    "dfeature      = fill_nan(dfeature)\n",
    "dfeature_test = fill_nan(dfeature_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcorr = dfeature.corr()\n",
    "colinearList = []\n",
    "for idx in dfcorr.index:\n",
    "    for col in dfcorr.columns:\n",
    "        if dfcorr.loc[idx,col] > 0.9 and idx!=col:\n",
    "            colinearList.append( [set([idx,col]) , dfcorr.loc[idx,col]] )\n",
    "(colinearList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model construction\n",
    "* the main model is xgboost(GBDT)\n",
    "* try to use the leaf id of xgboost/randomoforest as the input of logisticRegression/SVM (need more parameter tuning)\n",
    "* try ensemble some classic models such as MLP,SVM,NaiveBayesian and KNN (need more parameter tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     22,
     25,
     41,
     51,
     83
    ]
   },
   "outputs": [],
   "source": [
    "# feature engineering as a transformer to avoid touching the TARGET information while calculate woe\n",
    "from sklearn.base import BaseEstimator\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder,MinMaxScaler,StandardScaler,QuantileTransformer\n",
    "from sklearn.decomposition import PCA\n",
